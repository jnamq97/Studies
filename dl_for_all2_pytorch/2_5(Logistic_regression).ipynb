{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719990ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2710], requires_grad=True)]\n",
      "Epoch:   0/2000 Cost:31667.599609\n",
      "Epoch: 100/2000 Cost:0.225993\n",
      "Epoch: 200/2000 Cost:0.223911\n",
      "Epoch: 300/2000 Cost:0.221941\n",
      "Epoch: 400/2000 Cost:0.220059\n",
      "Epoch: 500/2000 Cost:0.218271\n",
      "Epoch: 600/2000 Cost:0.216575\n",
      "Epoch: 700/2000 Cost:0.214950\n",
      "Epoch: 800/2000 Cost:0.213413\n",
      "Epoch: 900/2000 Cost:0.211952\n",
      "Epoch:1000/2000 Cost:0.210559\n",
      "Epoch:1100/2000 Cost:0.209230\n",
      "Epoch:1200/2000 Cost:0.207967\n",
      "Epoch:1300/2000 Cost:0.206762\n",
      "Epoch:1400/2000 Cost:0.205618\n",
      "Epoch:1500/2000 Cost:0.204529\n",
      "Epoch:1600/2000 Cost:0.203481\n",
      "Epoch:1700/2000 Cost:0.202486\n",
      "Epoch:1800/2000 Cost:0.201539\n",
      "Epoch:1900/2000 Cost:0.200634\n",
      "Epoch:2000/2000 Cost:0.199770\n",
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "print(list(model.parameters()))\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0 :\n",
    "        print('Epoch:{:4d}/{} Cost:{:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "        \n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67f0cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.5435,  0.3462, -0.1188]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2937], requires_grad=True)]\n",
      "Epoch:   0/2000 Cost:39633.414062\n",
      "Epoch: 100/2000 Cost:11.480746\n",
      "Epoch: 200/2000 Cost:10.894592\n",
      "Epoch: 300/2000 Cost:10.339335\n",
      "Epoch: 400/2000 Cost:9.813351\n",
      "Epoch: 500/2000 Cost:9.315010\n",
      "Epoch: 600/2000 Cost:8.842962\n",
      "Epoch: 700/2000 Cost:8.395753\n",
      "Epoch: 800/2000 Cost:7.972028\n",
      "Epoch: 900/2000 Cost:7.570637\n",
      "Epoch:1000/2000 Cost:7.190376\n",
      "Epoch:1100/2000 Cost:6.830142\n",
      "Epoch:1200/2000 Cost:6.488811\n",
      "Epoch:1300/2000 Cost:6.165472\n",
      "Epoch:1400/2000 Cost:5.859105\n",
      "Epoch:1500/2000 Cost:5.568909\n",
      "Epoch:1600/2000 Cost:5.293931\n",
      "Epoch:1700/2000 Cost:5.033408\n",
      "Epoch:1800/2000 Cost:4.786575\n",
      "Epoch:1900/2000 Cost:4.552718\n",
      "Epoch:2000/2000 Cost:4.331151\n",
      "[Parameter containing:\n",
      "tensor([[0.4789, 0.8222, 0.7059]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3070], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# nn.Module의 클래스 구현\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "model = LinearRegressionModel()\n",
    "print(list(model.parameters()))\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    prediction = model.forward(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0 :\n",
    "        print('Epoch:{:4d}/{} Cost:{:.6f}'.format(epoch, nb_epochs, cost.item()))\n",
    "        \n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88556db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [89., 91., 90.]]), tensor([[185.],\n",
      "        [180.]])]\n",
      "Epoch: 0/20 Batch:1/3 Cost:8862.895508\n",
      "1\n",
      "[tensor([[73., 80., 75.],\n",
      "        [73., 66., 70.]]), tensor([[152.],\n",
      "        [142.]])]\n",
      "Epoch: 0/20 Batch:2/3 Cost:1461.758545\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch: 0/20 Batch:3/3 Cost:1149.987671\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch: 1/20 Batch:1/3 Cost:235.571991\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch: 1/20 Batch:2/3 Cost:37.515854\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch: 1/20 Batch:3/3 Cost:1.261714\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch: 2/20 Batch:1/3 Cost:15.263241\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch: 2/20 Batch:2/3 Cost:6.956800\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch: 2/20 Batch:3/3 Cost:0.289635\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch: 3/20 Batch:1/3 Cost:5.771740\n",
      "1\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch: 3/20 Batch:2/3 Cost:3.803086\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch: 3/20 Batch:3/3 Cost:1.716134\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch: 4/20 Batch:1/3 Cost:4.488634\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch: 4/20 Batch:2/3 Cost:1.630364\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch: 4/20 Batch:3/3 Cost:8.198117\n",
      "0\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch: 5/20 Batch:1/3 Cost:4.293013\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch: 5/20 Batch:2/3 Cost:3.342106\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch: 5/20 Batch:3/3 Cost:5.622592\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch: 6/20 Batch:1/3 Cost:5.147801\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch: 6/20 Batch:2/3 Cost:2.240922\n",
      "2\n",
      "[tensor([[89., 91., 90.]]), tensor([[180.]])]\n",
      "Epoch: 6/20 Batch:3/3 Cost:3.740319\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch: 7/20 Batch:1/3 Cost:4.288431\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 89.,  91.,  90.]]), tensor([[196.],\n",
      "        [180.]])]\n",
      "Epoch: 7/20 Batch:2/3 Cost:1.077333\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch: 7/20 Batch:3/3 Cost:8.166080\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 89.,  91.,  90.]]), tensor([[196.],\n",
      "        [180.]])]\n",
      "Epoch: 8/20 Batch:1/3 Cost:2.613321\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch: 8/20 Batch:2/3 Cost:4.264480\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch: 8/20 Batch:3/3 Cost:6.031155\n",
      "0\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch: 9/20 Batch:1/3 Cost:3.889950\n",
      "1\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch: 9/20 Batch:2/3 Cost:4.306903\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch: 9/20 Batch:3/3 Cost:5.512350\n",
      "0\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch:10/20 Batch:1/3 Cost:2.341461\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch:10/20 Batch:2/3 Cost:5.206254\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch:10/20 Batch:3/3 Cost:6.970161\n",
      "0\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch:11/20 Batch:1/3 Cost:2.221917\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch:11/20 Batch:2/3 Cost:4.810912\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch:11/20 Batch:3/3 Cost:7.184324\n",
      "0\n",
      "[tensor([[ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.]]), tensor([[180.],\n",
      "        [196.]])]\n",
      "Epoch:12/20 Batch:1/3 Cost:3.129806\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [73., 80., 75.]]), tensor([[142.],\n",
      "        [152.]])]\n",
      "Epoch:12/20 Batch:2/3 Cost:4.816163\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch:12/20 Batch:3/3 Cost:4.761287\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch:13/20 Batch:1/3 Cost:3.968792\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  80.,  75.]]), tensor([[196.],\n",
      "        [152.]])]\n",
      "Epoch:13/20 Batch:2/3 Cost:4.531684\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch:13/20 Batch:3/3 Cost:5.303288\n",
      "0\n",
      "[tensor([[73., 80., 75.],\n",
      "        [89., 91., 90.]]), tensor([[152.],\n",
      "        [180.]])]\n",
      "Epoch:14/20 Batch:1/3 Cost:6.074505\n",
      "1\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]]), tensor([[196.],\n",
      "        [142.]])]\n",
      "Epoch:14/20 Batch:2/3 Cost:3.423039\n",
      "2\n",
      "[tensor([[93., 88., 93.]]), tensor([[185.]])]\n",
      "Epoch:14/20 Batch:3/3 Cost:3.783231\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch:15/20 Batch:1/3 Cost:4.083768\n",
      "1\n",
      "[tensor([[ 93.,  88.,  93.],\n",
      "        [ 96.,  98., 100.]]), tensor([[185.],\n",
      "        [196.]])]\n",
      "Epoch:15/20 Batch:2/3 Cost:2.257290\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch:15/20 Batch:3/3 Cost:6.072747\n",
      "0\n",
      "[tensor([[ 73.,  80.,  75.],\n",
      "        [ 96.,  98., 100.]]), tensor([[152.],\n",
      "        [196.]])]\n",
      "Epoch:16/20 Batch:1/3 Cost:1.397101\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [93., 88., 93.]]), tensor([[180.],\n",
      "        [185.]])]\n",
      "Epoch:16/20 Batch:2/3 Cost:4.299688\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch:16/20 Batch:3/3 Cost:6.550853\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch:17/20 Batch:1/3 Cost:2.226663\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch:17/20 Batch:2/3 Cost:3.695060\n",
      "2\n",
      "[tensor([[73., 80., 75.]]), tensor([[152.]])]\n",
      "Epoch:17/20 Batch:3/3 Cost:5.491090\n",
      "0\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 66., 70.]]), tensor([[180.],\n",
      "        [142.]])]\n",
      "Epoch:18/20 Batch:1/3 Cost:4.107987\n",
      "1\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch:18/20 Batch:2/3 Cost:4.035447\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch:18/20 Batch:3/3 Cost:0.775940\n",
      "0\n",
      "[tensor([[ 96.,  98., 100.],\n",
      "        [ 93.,  88.,  93.]]), tensor([[196.],\n",
      "        [185.]])]\n",
      "Epoch:19/20 Batch:1/3 Cost:3.196944\n",
      "1\n",
      "[tensor([[89., 91., 90.],\n",
      "        [73., 80., 75.]]), tensor([[180.],\n",
      "        [152.]])]\n",
      "Epoch:19/20 Batch:2/3 Cost:2.936852\n",
      "2\n",
      "[tensor([[73., 66., 70.]]), tensor([[142.]])]\n",
      "Epoch:19/20 Batch:3/3 Cost:8.374249\n",
      "0\n",
      "[tensor([[93., 88., 93.],\n",
      "        [73., 80., 75.]]), tensor([[185.],\n",
      "        [152.]])]\n",
      "Epoch:20/20 Batch:1/3 Cost:3.888090\n",
      "1\n",
      "[tensor([[73., 66., 70.],\n",
      "        [89., 91., 90.]]), tensor([[142.],\n",
      "        [180.]])]\n",
      "Epoch:20/20 Batch:2/3 Cost:3.608489\n",
      "2\n",
      "[tensor([[ 96.,  98., 100.]]), tensor([[196.]])]\n",
      "Epoch:20/20 Batch:3/3 Cost:1.502323\n"
     ]
    }
   ],
   "source": [
    "# dataset, dataloader (batch를 나눠서 하기 위해 사용하는듯)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                            [93, 88, 93],\n",
    "                            [89, 91, 90],\n",
    "                            [96, 98, 100],\n",
    "                            [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "optimizer=optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        print(batch_idx)\n",
    "        print(samples)\n",
    "        x_train, y_train = samples\n",
    "        prediction = model(x_train)\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('Epoch:{:2d}/{} Batch:{}/{} Cost:{:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader), cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d1ad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/1000 Cost:0.693147\n",
      "Epoch: 100/1000 Cost:0.134722\n",
      "Epoch: 200/1000 Cost:0.080643\n",
      "Epoch: 300/1000 Cost:0.057900\n",
      "Epoch: 400/1000 Cost:0.045300\n",
      "Epoch: 500/1000 Cost:0.037261\n",
      "Epoch: 600/1000 Cost:0.031673\n",
      "Epoch: 700/1000 Cost:0.027556\n",
      "Epoch: 800/1000 Cost:0.024394\n",
      "Epoch: 900/1000 Cost:0.021888\n",
      "Epoch:1000/1000 Cost:0.019852\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "x_train = torch.FloatTensor([[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]])\n",
    "y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n",
    "\n",
    "W = torch.zeros((2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W,b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "    cost = -(y_train*torch.log(hypothesis) + (1 - y_train)*torch.log(1 - hypothesis)).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%100==0 : \n",
    "        print('Epoch:{:4d}/{} Cost:{:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584028c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0/1000 Cost:0.994583 Accuracy:50.00%\n",
      "Epoch:  10/1000 Cost:0.411348 Accuracy:66.67%\n",
      "Epoch:  20/1000 Cost:0.430951 Accuracy:83.33%\n",
      "Epoch:  30/1000 Cost:0.360431 Accuracy:83.33%\n",
      "Epoch:  40/1000 Cost:0.305088 Accuracy:83.33%\n",
      "Epoch:  50/1000 Cost:0.255869 Accuracy:83.33%\n",
      "Epoch:  60/1000 Cost:0.211406 Accuracy:100.00%\n",
      "Epoch:  70/1000 Cost:0.175426 Accuracy:100.00%\n",
      "Epoch:  80/1000 Cost:0.153818 Accuracy:100.00%\n",
      "Epoch:  90/1000 Cost:0.141830 Accuracy:100.00%\n",
      "Epoch: 100/1000 Cost:0.132340 Accuracy:100.00%\n",
      "Epoch: 110/1000 Cost:0.124077 Accuracy:100.00%\n",
      "Epoch: 120/1000 Cost:0.116803 Accuracy:100.00%\n",
      "Epoch: 130/1000 Cost:0.110351 Accuracy:100.00%\n",
      "Epoch: 140/1000 Cost:0.104589 Accuracy:100.00%\n",
      "Epoch: 150/1000 Cost:0.099411 Accuracy:100.00%\n",
      "Epoch: 160/1000 Cost:0.094734 Accuracy:100.00%\n",
      "Epoch: 170/1000 Cost:0.090487 Accuracy:100.00%\n",
      "Epoch: 180/1000 Cost:0.086613 Accuracy:100.00%\n",
      "Epoch: 190/1000 Cost:0.083066 Accuracy:100.00%\n",
      "Epoch: 200/1000 Cost:0.079805 Accuracy:100.00%\n",
      "Epoch: 210/1000 Cost:0.076796 Accuracy:100.00%\n",
      "Epoch: 220/1000 Cost:0.074012 Accuracy:100.00%\n",
      "Epoch: 230/1000 Cost:0.071428 Accuracy:100.00%\n",
      "Epoch: 240/1000 Cost:0.069022 Accuracy:100.00%\n",
      "Epoch: 250/1000 Cost:0.066777 Accuracy:100.00%\n",
      "Epoch: 260/1000 Cost:0.064677 Accuracy:100.00%\n",
      "Epoch: 270/1000 Cost:0.062708 Accuracy:100.00%\n",
      "Epoch: 280/1000 Cost:0.060858 Accuracy:100.00%\n",
      "Epoch: 290/1000 Cost:0.059116 Accuracy:100.00%\n",
      "Epoch: 300/1000 Cost:0.057474 Accuracy:100.00%\n",
      "Epoch: 310/1000 Cost:0.055923 Accuracy:100.00%\n",
      "Epoch: 320/1000 Cost:0.054455 Accuracy:100.00%\n",
      "Epoch: 330/1000 Cost:0.053063 Accuracy:100.00%\n",
      "Epoch: 340/1000 Cost:0.051743 Accuracy:100.00%\n",
      "Epoch: 350/1000 Cost:0.050488 Accuracy:100.00%\n",
      "Epoch: 360/1000 Cost:0.049293 Accuracy:100.00%\n",
      "Epoch: 370/1000 Cost:0.048155 Accuracy:100.00%\n",
      "Epoch: 380/1000 Cost:0.047069 Accuracy:100.00%\n",
      "Epoch: 390/1000 Cost:0.046033 Accuracy:100.00%\n",
      "Epoch: 400/1000 Cost:0.045041 Accuracy:100.00%\n",
      "Epoch: 410/1000 Cost:0.044093 Accuracy:100.00%\n",
      "Epoch: 420/1000 Cost:0.043184 Accuracy:100.00%\n",
      "Epoch: 430/1000 Cost:0.042312 Accuracy:100.00%\n",
      "Epoch: 440/1000 Cost:0.041476 Accuracy:100.00%\n",
      "Epoch: 450/1000 Cost:0.040672 Accuracy:100.00%\n",
      "Epoch: 460/1000 Cost:0.039900 Accuracy:100.00%\n",
      "Epoch: 470/1000 Cost:0.039157 Accuracy:100.00%\n",
      "Epoch: 480/1000 Cost:0.038441 Accuracy:100.00%\n",
      "Epoch: 490/1000 Cost:0.037752 Accuracy:100.00%\n",
      "Epoch: 500/1000 Cost:0.037087 Accuracy:100.00%\n",
      "Epoch: 510/1000 Cost:0.036446 Accuracy:100.00%\n",
      "Epoch: 520/1000 Cost:0.035826 Accuracy:100.00%\n",
      "Epoch: 530/1000 Cost:0.035228 Accuracy:100.00%\n",
      "Epoch: 540/1000 Cost:0.034650 Accuracy:100.00%\n",
      "Epoch: 550/1000 Cost:0.034091 Accuracy:100.00%\n",
      "Epoch: 560/1000 Cost:0.033549 Accuracy:100.00%\n",
      "Epoch: 570/1000 Cost:0.033025 Accuracy:100.00%\n",
      "Epoch: 580/1000 Cost:0.032517 Accuracy:100.00%\n",
      "Epoch: 590/1000 Cost:0.032025 Accuracy:100.00%\n",
      "Epoch: 600/1000 Cost:0.031547 Accuracy:100.00%\n",
      "Epoch: 610/1000 Cost:0.031084 Accuracy:100.00%\n",
      "Epoch: 620/1000 Cost:0.030635 Accuracy:100.00%\n",
      "Epoch: 630/1000 Cost:0.030198 Accuracy:100.00%\n",
      "Epoch: 640/1000 Cost:0.029774 Accuracy:100.00%\n",
      "Epoch: 650/1000 Cost:0.029361 Accuracy:100.00%\n",
      "Epoch: 660/1000 Cost:0.028960 Accuracy:100.00%\n",
      "Epoch: 670/1000 Cost:0.028570 Accuracy:100.00%\n",
      "Epoch: 680/1000 Cost:0.028191 Accuracy:100.00%\n",
      "Epoch: 690/1000 Cost:0.027821 Accuracy:100.00%\n",
      "Epoch: 700/1000 Cost:0.027462 Accuracy:100.00%\n",
      "Epoch: 710/1000 Cost:0.027111 Accuracy:100.00%\n",
      "Epoch: 720/1000 Cost:0.026769 Accuracy:100.00%\n",
      "Epoch: 730/1000 Cost:0.026436 Accuracy:100.00%\n",
      "Epoch: 740/1000 Cost:0.026112 Accuracy:100.00%\n",
      "Epoch: 750/1000 Cost:0.025795 Accuracy:100.00%\n",
      "Epoch: 760/1000 Cost:0.025486 Accuracy:100.00%\n",
      "Epoch: 770/1000 Cost:0.025184 Accuracy:100.00%\n",
      "Epoch: 780/1000 Cost:0.024889 Accuracy:100.00%\n",
      "Epoch: 790/1000 Cost:0.024602 Accuracy:100.00%\n",
      "Epoch: 800/1000 Cost:0.024320 Accuracy:100.00%\n",
      "Epoch: 810/1000 Cost:0.024046 Accuracy:100.00%\n",
      "Epoch: 820/1000 Cost:0.023777 Accuracy:100.00%\n",
      "Epoch: 830/1000 Cost:0.023515 Accuracy:100.00%\n",
      "Epoch: 840/1000 Cost:0.023258 Accuracy:100.00%\n",
      "Epoch: 850/1000 Cost:0.023007 Accuracy:100.00%\n",
      "Epoch: 860/1000 Cost:0.022761 Accuracy:100.00%\n",
      "Epoch: 870/1000 Cost:0.022521 Accuracy:100.00%\n",
      "Epoch: 880/1000 Cost:0.022285 Accuracy:100.00%\n",
      "Epoch: 890/1000 Cost:0.022055 Accuracy:100.00%\n",
      "Epoch: 900/1000 Cost:0.021829 Accuracy:100.00%\n",
      "Epoch: 910/1000 Cost:0.021608 Accuracy:100.00%\n",
      "Epoch: 920/1000 Cost:0.021391 Accuracy:100.00%\n",
      "Epoch: 930/1000 Cost:0.021179 Accuracy:100.00%\n",
      "Epoch: 940/1000 Cost:0.020971 Accuracy:100.00%\n",
      "Epoch: 950/1000 Cost:0.020767 Accuracy:100.00%\n",
      "Epoch: 960/1000 Cost:0.020566 Accuracy:100.00%\n",
      "Epoch: 970/1000 Cost:0.020370 Accuracy:100.00%\n",
      "Epoch: 980/1000 Cost:0.020178 Accuracy:100.00%\n",
      "Epoch: 990/1000 Cost:0.019989 Accuracy:100.00%\n",
      "Epoch:1000/1000 Cost:0.019803 Accuracy:100.00%\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression by nn.Module\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x_train = torch.FloatTensor([[1,2], [2,3], [3,1], [4,3], [5,3], [6,2]])\n",
    "y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n",
    "\n",
    "model = nn.Sequential(nn.Linear(2,1), nn.Sigmoid())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    predicted_y = model(x_train)\n",
    "    cost = F.binary_cross_entropy(predicted_y, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%10==0:\n",
    "        prediction = predicted_y >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch:{:4d}/{} Cost:{:.6f} Accuracy:{:2.2f}%'.format(epoch, nb_epochs, cost.item(), accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd23cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
